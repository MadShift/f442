{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "206c09d5",
   "metadata": {
    "id": "206c09d5",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#!pip install datasets evaluate transformers rouge-score nltk accelerate bitsandbytes sacrebleu tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cad23f89-8252-472c-ad78-af92c9824005",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.environ['HTTP_PROXY'] = 'http://pechenov.sv:8toFf%3AMVT+Cb@proxy.techpark.local:8080'\n",
    "os.environ['HTTPS_PROXY'] = 'http://pechenov.sv:8toFf%3AMVT+Cb@proxy.techpark.local:8080'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7f922af0",
   "metadata": {
    "id": "7f922af0"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /home/pechenov.sv@techpark.local/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5d9afe70",
   "metadata": {
    "id": "5d9afe70",
    "outputId": "1942bf29-ad3d-459d-a3a9-e6d14e090aaa"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/pechenov.sv@techpark.local/.local/lib/python3.8/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import transformers\n",
    "\n",
    "model_checkpoint = \"ai-forever/FRED-T5-large\"\n",
    "#model_checkpoint = \"t5-small\"\n",
    "\n",
    "#print(transformers.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d39331c8",
   "metadata": {
    "id": "d39331c8"
   },
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "#from evaluate import load\n",
    "\n",
    "#dataset = load_dataset(\"composite/pauq\", \"ru_pauq_iid\", split='train[:5%]')\n",
    "dataset = load_dataset(\"composite/pauq\", \"ru_pauq_tl\")\n",
    "#metric = load(\"exact_match\")\n",
    "#dataset = dataset.train_test_split(test_size=0.2)\n",
    "#dataset\n",
    "#results = metric.compute(predictions=predictions, references=references)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "af51d8d4",
   "metadata": {
    "id": "af51d8d4"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GPT2TokenizerFast(name_or_path='ai-forever/FRED-T5-large', vocab_size=50257, model_max_length=1000000000000000019884624838656, is_fast=True, padding_side='right', truncation_side='right', special_tokens={'bos_token': AddedToken(\"<s>\", rstrip=False, lstrip=False, single_word=False, normalized=True), 'eos_token': AddedToken(\"</s>\", rstrip=False, lstrip=False, single_word=False, normalized=True), 'unk_token': AddedToken(\"<|endoftext|>\", rstrip=False, lstrip=False, single_word=False, normalized=True), 'pad_token': '<pad>'}, clean_up_tokenization_spaces=True)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)\n",
    "tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "21cee046-88fb-4e4e-8e69-28f1e47ac2f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!echo $PATH\n",
    "#!export PATH=\"$PATH:/usr/bin\"\n",
    "#!echo $PATH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7fdd8750-bba1-4149-9c53-4c7d21cf5a14",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/pechenov.sv@techpark.local/.local/lib/python3.8/site-packages/gdown/cli.py:126: FutureWarning: Option `--id` was deprecated in version 4.3.1 and will be removed in 5.0. You don't need to pass it anymore to use a file ID.\n",
      "  warnings.warn(\n",
      "Downloading...\n",
      "From (uriginal): https://drive.google.com/uc?id=1Xjbp207zfCaBxhPgt-STB_RxwNo2TIW2\n",
      "From (redirected): https://drive.google.com/uc?id=1Xjbp207zfCaBxhPgt-STB_RxwNo2TIW2&confirm=t&uuid=3257c612-fd17-4fd7-b2ca-0081a9fb0902\n",
      "To: /home/pechenov.sv@techpark.local/merged_database_2022-06-10.zip\n",
      "100%|████████████████████████████████████████| 325M/325M [00:03<00:00, 95.4MB/s]\n",
      "Archive:  merged_database_2022-06-10.zip\n",
      "  inflating: data/binary.xlsx        \n",
      "  inflating: data/dates.xlsx         \n",
      "  inflating: data/dev_to_check.xlsx  \n",
      "  inflating: data/english_for_remark_NONE&DBP.xlsx  \n",
      "  inflating: data/fuzzy.xlsx         \n",
      "   creating: data/merged_database/\n",
      "  inflating: data/merged_database/.DS_Store  \n",
      "   creating: data/merged_database/academic/\n",
      "  inflating: data/merged_database/academic/academic.sqlite  \n",
      "  inflating: data/merged_database/academic/schema.sql  \n",
      "   creating: data/merged_database/activity_1/\n",
      "  inflating: data/merged_database/activity_1/activity_1.sqlite  \n",
      "  inflating: data/merged_database/activity_1/schema.sql  \n",
      "   creating: data/merged_database/aircraft/\n",
      "  inflating: data/merged_database/aircraft/aircraft.sqlite  \n",
      "  inflating: data/merged_database/aircraft/schema.sql  \n",
      "   creating: data/merged_database/allergy_1/\n",
      "  inflating: data/merged_database/allergy_1/allergy_1.sqlite  \n",
      "  inflating: data/merged_database/allergy_1/schema.sql  \n",
      "   creating: data/merged_database/apartment_rentals/\n",
      "  inflating: data/merged_database/apartment_rentals/apartment_rentals.sqlite  \n",
      "  inflating: data/merged_database/apartment_rentals/schema.sql  \n",
      "   creating: data/merged_database/architecture/\n",
      "  inflating: data/merged_database/architecture/architecture.sqlite  \n",
      "  inflating: data/merged_database/architecture/schema.sql  \n",
      "   creating: data/merged_database/assets_maintenance/\n",
      "  inflating: data/merged_database/assets_maintenance/assets_maintenance.sqlite  \n",
      "  inflating: data/merged_database/assets_maintenance/schema.sql  \n",
      "   creating: data/merged_database/baseball_1/\n",
      "  inflating: data/merged_database/baseball_1/baseball_1.sqlite  \n",
      "  inflating: data/merged_database/baseball_1/schema.sql  \n",
      "   creating: data/merged_database/battle_death/\n",
      "  inflating: data/merged_database/battle_death/battle_death.sqlite  \n",
      "  inflating: data/merged_database/battle_death/schema.sql  \n",
      "   creating: data/merged_database/behavior_monitoring/\n",
      "  inflating: data/merged_database/behavior_monitoring/behavior_monitoring.sqlite  \n",
      "  inflating: data/merged_database/behavior_monitoring/schema.sql  \n",
      "   creating: data/merged_database/bike_1/\n",
      "  inflating: data/merged_database/bike_1/bike_1.sqlite  \n",
      "  inflating: data/merged_database/bike_1/schema.sql  \n",
      "   creating: data/merged_database/body_builder/\n",
      "  inflating: data/merged_database/body_builder/body_builder.sqlite  \n",
      "  inflating: data/merged_database/body_builder/schema.sql  \n",
      "   creating: data/merged_database/book_2/\n",
      "  inflating: data/merged_database/book_2/book_2.sqlite  \n",
      "  inflating: data/merged_database/book_2/schema.sql  \n",
      "   creating: data/merged_database/browser_web/\n",
      "  inflating: data/merged_database/browser_web/browser_web.sqlite  \n",
      "  inflating: data/merged_database/browser_web/schema.sql  \n",
      "   creating: data/merged_database/candidate_poll/\n",
      "  inflating: data/merged_database/candidate_poll/candidate_poll.sqlite  \n",
      "  inflating: data/merged_database/candidate_poll/schema.sql  \n",
      "   creating: data/merged_database/car_1/\n",
      "  inflating: data/merged_database/car_1/annotation.json  \n",
      "  inflating: data/merged_database/car_1/car_1.json  \n",
      "  inflating: data/merged_database/car_1/car_1.sql  \n",
      "  inflating: data/merged_database/car_1/car_1.sqlite  \n",
      "   creating: data/merged_database/car_1/data_csv/\n",
      "  inflating: data/merged_database/car_1/data_csv/car-makers.csv  \n",
      "  inflating: data/merged_database/car_1/data_csv/car-names.csv  \n",
      "  inflating: data/merged_database/car_1/data_csv/cars-data.csv  \n",
      "  inflating: data/merged_database/car_1/data_csv/cars.desc  \n",
      "  inflating: data/merged_database/car_1/data_csv/continents.csv  \n",
      "  inflating: data/merged_database/car_1/data_csv/countries.csv  \n",
      "  inflating: data/merged_database/car_1/data_csv/model-list.csv  \n",
      "  inflating: data/merged_database/car_1/data_csv/README.CARS.TXT  \n",
      " extracting: data/merged_database/car_1/link.txt  \n",
      "  inflating: data/merged_database/car_1/q.txt  \n",
      "   creating: data/merged_database/chinook_1/\n",
      "  inflating: data/merged_database/chinook_1/annotation.json  \n",
      "  inflating: data/merged_database/chinook_1/chinook_1.sqlite  \n",
      "   creating: data/merged_database/cinema/\n",
      "  inflating: data/merged_database/cinema/cinema.sqlite  \n",
      "  inflating: data/merged_database/cinema/schema.sql  \n",
      "   creating: data/merged_database/city_record/\n",
      "  inflating: data/merged_database/city_record/city_record.sqlite  \n",
      "  inflating: data/merged_database/city_record/schema.sql  \n",
      "   creating: data/merged_database/climbing/\n",
      "  inflating: data/merged_database/climbing/climbing.sqlite  \n",
      "  inflating: data/merged_database/climbing/schema.sql  \n",
      "   creating: data/merged_database/club_1/\n",
      "  inflating: data/merged_database/club_1/club_1.sqlite  \n",
      "  inflating: data/merged_database/club_1/schema.sql  \n",
      "   creating: data/merged_database/coffee_shop/\n",
      "  inflating: data/merged_database/coffee_shop/coffee_shop.sqlite  \n",
      "  inflating: data/merged_database/coffee_shop/schema.sql  \n",
      "   creating: data/merged_database/college_1/\n",
      "  inflating: data/merged_database/college_1/college_1.sqlite  \n",
      " extracting: data/merged_database/college_1/link.txt  \n",
      "  inflating: data/merged_database/college_1/TinyCollege.sql  \n",
      "   creating: data/merged_database/college_2/\n",
      "  inflating: data/merged_database/college_2/college_2.sqlite  \n",
      " extracting: data/merged_database/college_2/link.txt  \n",
      "  inflating: data/merged_database/college_2/TextBookExampleSchema.sql  \n",
      "   creating: data/merged_database/college_3/\n",
      "  inflating: data/merged_database/college_3/college_3.sqlite  \n",
      "  inflating: data/merged_database/college_3/schema.sql  \n",
      "   creating: data/merged_database/company_1/\n",
      "  inflating: data/merged_database/company_1/company_1.sqlite  \n",
      "  inflating: data/merged_database/company_1/link.txt  \n",
      "   creating: data/merged_database/company_employee/\n",
      "  inflating: data/merged_database/company_employee/company_employee.sqlite  \n",
      "  inflating: data/merged_database/company_employee/schema.sql  \n",
      "   creating: data/merged_database/company_office/\n",
      "  inflating: data/merged_database/company_office/company_office.sqlite  \n",
      "  inflating: data/merged_database/company_office/schema.sql  \n",
      "   creating: data/merged_database/concert_singer/\n",
      "  inflating: data/merged_database/concert_singer/concert_singer.sqlite  \n",
      "  inflating: data/merged_database/concert_singer/schema.sql  \n",
      "   creating: data/merged_database/county_public_safety/\n",
      "  inflating: data/merged_database/county_public_safety/county_public_safety.sqlite  \n",
      "  inflating: data/merged_database/county_public_safety/schema.sql  \n",
      "   creating: data/merged_database/course_teach/\n",
      "  inflating: data/merged_database/course_teach/course_teach.sqlite  \n",
      "  inflating: data/merged_database/course_teach/schema.sql  \n",
      "   creating: data/merged_database/cre_Docs_and_Epenses/\n",
      "  inflating: data/merged_database/cre_Docs_and_Epenses/cre_Docs_and_Epenses.sqlite  \n",
      "  inflating: data/merged_database/cre_Docs_and_Epenses/schema.sql  \n",
      "   creating: data/merged_database/cre_Doc_Control_Systems/\n",
      "  inflating: data/merged_database/cre_Doc_Control_Systems/cre_Doc_Control_Systems.sqlite  \n",
      "  inflating: data/merged_database/cre_Doc_Control_Systems/schema.sql  \n",
      "   creating: data/merged_database/cre_Doc_Template_Mgt/\n",
      "  inflating: data/merged_database/cre_Doc_Template_Mgt/cre_Doc_Template_Mgt.sqlite  \n",
      "  inflating: data/merged_database/cre_Doc_Template_Mgt/schema.sql  \n",
      "   creating: data/merged_database/cre_Doc_Tracking_DB/\n",
      "  inflating: data/merged_database/cre_Doc_Tracking_DB/cre_Doc_Tracking_DB.sqlite  \n",
      "  inflating: data/merged_database/cre_Doc_Tracking_DB/schema.sql  \n",
      "   creating: data/merged_database/cre_Drama_Workshop_Groups/\n",
      "  inflating: data/merged_database/cre_Drama_Workshop_Groups/cre_Drama_Workshop_Groups.sqlite  \n",
      "  inflating: data/merged_database/cre_Drama_Workshop_Groups/schema.sql  \n",
      "   creating: data/merged_database/cre_Theme_park/\n",
      "  inflating: data/merged_database/cre_Theme_park/cre_Theme_park.sqlite  \n",
      "  inflating: data/merged_database/cre_Theme_park/schema.sql  \n",
      "   creating: data/merged_database/csu_1/\n",
      "  inflating: data/merged_database/csu_1/csu_1.sqlite  \n",
      "  inflating: data/merged_database/csu_1/schema.sql  \n",
      "   creating: data/merged_database/culture_company/\n",
      "  inflating: data/merged_database/culture_company/culture_company.sqlite  \n",
      "  inflating: data/merged_database/culture_company/schema.sql  \n",
      "   creating: data/merged_database/customers_and_addresses/\n",
      "  inflating: data/merged_database/customers_and_addresses/customers_and_addresses.sqlite  \n",
      "  inflating: data/merged_database/customers_and_addresses/schema.sql  \n",
      "   creating: data/merged_database/customers_and_invoices/\n",
      "  inflating: data/merged_database/customers_and_invoices/customers_and_invoices.sqlite  \n",
      "  inflating: data/merged_database/customers_and_invoices/schema.sql  \n",
      "   creating: data/merged_database/customers_and_products_contacts/\n",
      "  inflating: data/merged_database/customers_and_products_contacts/customers_and_products_contacts.sqlite  \n",
      "  inflating: data/merged_database/customers_and_products_contacts/schema.sql  \n",
      "   creating: data/merged_database/customers_campaigns_ecommerce/\n",
      "  inflating: data/merged_database/customers_campaigns_ecommerce/customers_campaigns_ecommerce.sqlite  \n",
      "  inflating: data/merged_database/customers_campaigns_ecommerce/schema.sql  \n",
      "   creating: data/merged_database/customers_card_transactions/\n",
      "  inflating: data/merged_database/customers_card_transactions/customers_card_transactions.sqlite  \n",
      "  inflating: data/merged_database/customers_card_transactions/schema.sql  \n",
      "   creating: data/merged_database/customer_complaints/\n",
      "  inflating: data/merged_database/customer_complaints/customer_complaints.sqlite  \n",
      "  inflating: data/merged_database/customer_complaints/schema.sql  \n",
      "   creating: data/merged_database/customer_deliveries/\n",
      "  inflating: data/merged_database/customer_deliveries/customer_deliveries.sqlite  \n",
      "  inflating: data/merged_database/customer_deliveries/schema.sql  \n",
      "   creating: data/merged_database/debate/\n",
      "  inflating: data/merged_database/debate/debate.sqlite  \n",
      "  inflating: data/merged_database/debate/schema.sql  \n",
      "   creating: data/merged_database/decoration_competition/\n",
      "  inflating: data/merged_database/decoration_competition/decoration_competition.sqlite  \n",
      "  inflating: data/merged_database/decoration_competition/schema.sql  \n",
      "   creating: data/merged_database/department_management/\n",
      "  inflating: data/merged_database/department_management/department_management.sqlite  \n",
      "  inflating: data/merged_database/department_management/schema.sql  \n",
      "   creating: data/merged_database/department_store/\n",
      "  inflating: data/merged_database/department_store/department_store.sqlite  \n",
      "  inflating: data/merged_database/department_store/schema.sql  \n",
      "   creating: data/merged_database/device/\n",
      "  inflating: data/merged_database/device/device.sqlite  \n",
      "  inflating: data/merged_database/device/schema.sql  \n",
      "   creating: data/merged_database/document_management/\n",
      "  inflating: data/merged_database/document_management/document_management.sqlite  \n",
      "  inflating: data/merged_database/document_management/schema.sql  \n",
      "   creating: data/merged_database/dog_kennels/\n",
      "  inflating: data/merged_database/dog_kennels/dog_kennels.sqlite  \n",
      "  inflating: data/merged_database/dog_kennels/schema.sql  \n",
      "   creating: data/merged_database/dorm_1/\n",
      "  inflating: data/merged_database/dorm_1/dorm_1.sqlite  \n",
      "  inflating: data/merged_database/dorm_1/schema.sql  \n",
      "   creating: data/merged_database/driving_school/\n",
      "  inflating: data/merged_database/driving_school/driving_school.sqlite  \n",
      "  inflating: data/merged_database/driving_school/schema.sql  \n",
      "   creating: data/merged_database/election/\n",
      "  inflating: data/merged_database/election/election.sqlite  \n",
      "  inflating: data/merged_database/election/schema.sql  \n",
      "   creating: data/merged_database/election_representative/\n",
      "  inflating: data/merged_database/election_representative/election_representative.sqlite  \n",
      "  inflating: data/merged_database/election_representative/schema.sql  \n",
      "   creating: data/merged_database/employee_hire_evaluation/\n",
      "  inflating: data/merged_database/employee_hire_evaluation/employee_hire_evaluation.sqlite  \n",
      "  inflating: data/merged_database/employee_hire_evaluation/schema.sql  \n",
      "   creating: data/merged_database/entertainment_awards/\n",
      "  inflating: data/merged_database/entertainment_awards/entertainment_awards.sqlite  \n",
      "  inflating: data/merged_database/entertainment_awards/schema.sql  \n",
      "   creating: data/merged_database/entrepreneur/\n",
      "  inflating: data/merged_database/entrepreneur/entrepreneur.sqlite  \n",
      "  inflating: data/merged_database/entrepreneur/schema.sql  \n",
      "   creating: data/merged_database/epinions_1/\n",
      "  inflating: data/merged_database/epinions_1/epinions_1.sqlite  \n",
      "   creating: data/merged_database/e_government/\n",
      "  inflating: data/merged_database/e_government/e_government.sqlite  \n",
      "  inflating: data/merged_database/e_government/schema.sql  \n",
      "   creating: data/merged_database/e_learning/\n",
      "  inflating: data/merged_database/e_learning/e_learning.sqlite  \n",
      "  inflating: data/merged_database/e_learning/schema.sql  \n",
      "   creating: data/merged_database/farm/\n",
      "  inflating: data/merged_database/farm/farm.sqlite  \n",
      "  inflating: data/merged_database/farm/schema.sql  \n",
      "   creating: data/merged_database/film_rank/\n",
      "  inflating: data/merged_database/film_rank/film_rank.sqlite  \n",
      "  inflating: data/merged_database/film_rank/schema.sql  \n",
      "   creating: data/merged_database/flight_1/\n",
      "  inflating: data/merged_database/flight_1/flight_1.sqlite  \n",
      "  inflating: data/merged_database/flight_1/schema.sql  \n",
      "   creating: data/merged_database/flight_2/\n",
      "  inflating: data/merged_database/flight_2/annotation.json  \n",
      "   creating: data/merged_database/flight_2/data_csv/\n",
      "  inflating: data/merged_database/flight_2/data_csv/airlines.csv  \n",
      "  inflating: data/merged_database/flight_2/data_csv/airports100.csv  \n",
      "  inflating: data/merged_database/flight_2/data_csv/flights.csv  \n",
      "  inflating: data/merged_database/flight_2/data_csv/README.AIRLINES.txt  \n",
      "  inflating: data/merged_database/flight_2/flight_2.json  \n",
      "  inflating: data/merged_database/flight_2/flight_2.sql  \n",
      "  inflating: data/merged_database/flight_2/flight_2.sqlite  \n",
      " extracting: data/merged_database/flight_2/link.txt  \n",
      "  inflating: data/merged_database/flight_2/q.txt  \n",
      "   creating: data/merged_database/flight_4/\n",
      "  inflating: data/merged_database/flight_4/flight_4.sqlite  \n",
      " extracting: data/merged_database/flight_4/link.txt  \n",
      " extracting: data/merged_database/flight_4/sql.txt  \n",
      "   creating: data/merged_database/flight_company/\n",
      "  inflating: data/merged_database/flight_company/flight_company.sqlite  \n",
      "  inflating: data/merged_database/flight_company/schema.sql  \n",
      "   creating: data/merged_database/formula_1/\n",
      "  inflating: data/merged_database/formula_1/annotation.json  \n",
      "   creating: data/merged_database/formula_1/data_csv/\n",
      "  inflating: data/merged_database/formula_1/data_csv/circuits.csv  \n",
      "  inflating: data/merged_database/formula_1/data_csv/constructorResults.csv  \n",
      "  inflating: data/merged_database/formula_1/data_csv/constructors.csv  \n",
      "  inflating: data/merged_database/formula_1/data_csv/constructorStandings.csv  \n",
      "  inflating: data/merged_database/formula_1/data_csv/drivers.csv  \n",
      "  inflating: data/merged_database/formula_1/data_csv/driverStandings.csv  \n",
      "  inflating: data/merged_database/formula_1/data_csv/lapTimes.csv  \n",
      "  inflating: data/merged_database/formula_1/data_csv/pitStops.csv  \n",
      "  inflating: data/merged_database/formula_1/data_csv/qualifying.csv  \n",
      "  inflating: data/merged_database/formula_1/data_csv/races.csv  \n",
      "  inflating: data/merged_database/formula_1/data_csv/results.csv  \n",
      "  inflating: data/merged_database/formula_1/data_csv/seasons.csv  \n",
      "  inflating: data/merged_database/formula_1/data_csv/status.csv  \n",
      " extracting: data/merged_database/formula_1/formula_1.splite  \n",
      "  inflating: data/merged_database/formula_1/formula_1.sql  \n",
      "  inflating: data/merged_database/formula_1/formula_1.sqlite  \n",
      "   creating: data/merged_database/game_1/\n",
      "  inflating: data/merged_database/game_1/game_1.sqlite  \n",
      "  inflating: data/merged_database/game_1/schema.sql  \n",
      "   creating: data/merged_database/game_injury/\n",
      "  inflating: data/merged_database/game_injury/game_injury.sqlite  \n",
      "  inflating: data/merged_database/game_injury/schema.sql  \n",
      "   creating: data/merged_database/gas_company/\n",
      "  inflating: data/merged_database/gas_company/gas_company.sqlite  \n",
      "  inflating: data/merged_database/gas_company/schema.sql  \n",
      "   creating: data/merged_database/geo/\n",
      "  inflating: data/merged_database/geo/geo.sqlite  \n",
      "  inflating: data/merged_database/geo/schema.sql  \n",
      "   creating: data/merged_database/gymnast/\n",
      "  inflating: data/merged_database/gymnast/gymnast.sqlite  \n",
      "  inflating: data/merged_database/gymnast/schema.sql  \n",
      "   creating: data/merged_database/hospital_1/\n",
      "  inflating: data/merged_database/hospital_1/hospital_1.sqlite  \n",
      "  inflating: data/merged_database/hospital_1/schema.sql  \n",
      "   creating: data/merged_database/hr_1/\n",
      "  inflating: data/merged_database/hr_1/hr_1.sqlite  \n",
      "  inflating: data/merged_database/hr_1/schema.sql  \n",
      "   creating: data/merged_database/icfp_1/\n",
      "  inflating: data/merged_database/icfp_1/icfp_1.sqlite  \n",
      "  inflating: data/merged_database/icfp_1/link.txt  \n",
      "  inflating: data/merged_database/icfp_1/q.txt  \n",
      "   creating: data/merged_database/imdb/\n",
      "  inflating: data/merged_database/imdb/imdb.sqlite  \n",
      "  inflating: data/merged_database/imdb/schema.sql  \n",
      "   creating: data/merged_database/inn_1/\n",
      "  inflating: data/merged_database/inn_1/annotation.json  \n",
      "  inflating: data/merged_database/inn_1/change_date.py  \n",
      "   creating: data/merged_database/inn_1/data_csv/\n",
      "  inflating: data/merged_database/inn_1/data_csv/README.INN.TXT  \n",
      "  inflating: data/merged_database/inn_1/data_csv/Reservations.csv  \n",
      "  inflating: data/merged_database/inn_1/data_csv/Reservations_t.csv  \n",
      "  inflating: data/merged_database/inn_1/data_csv/Rooms.csv  \n",
      "  inflating: data/merged_database/inn_1/inn_1.sql  \n",
      "  inflating: data/merged_database/inn_1/inn_1.sqlite  \n",
      " extracting: data/merged_database/inn_1/link.txt  \n",
      "  inflating: data/merged_database/inn_1/q.txt  \n",
      "   creating: data/merged_database/insurance_and_eClaims/\n",
      "  inflating: data/merged_database/insurance_and_eClaims/insurance_and_eClaims.sqlite  \n",
      "  inflating: data/merged_database/insurance_and_eClaims/schema.sql  \n",
      "   creating: data/merged_database/insurance_fnol/\n",
      "  inflating: data/merged_database/insurance_fnol/insurance_fnol.sqlite  \n",
      "  inflating: data/merged_database/insurance_fnol/schema.sql  \n",
      "   creating: data/merged_database/insurance_policies/\n",
      "  inflating: data/merged_database/insurance_policies/insurance_policies.sqlite  \n",
      "  inflating: data/merged_database/insurance_policies/schema.sql  \n",
      "   creating: data/merged_database/journal_committee/\n",
      "  inflating: data/merged_database/journal_committee/journal_committee.sqlite  \n",
      "  inflating: data/merged_database/journal_committee/schema.sql  \n",
      "   creating: data/merged_database/loan_1/\n",
      "  inflating: data/merged_database/loan_1/loan_1.sqlite  \n",
      "  inflating: data/merged_database/loan_1/schema.sql  \n",
      "   creating: data/merged_database/local_govt_and_lot/\n",
      "  inflating: data/merged_database/local_govt_and_lot/local_govt_and_lot.sqlite  \n",
      "  inflating: data/merged_database/local_govt_and_lot/schema.sql  \n",
      "   creating: data/merged_database/local_govt_in_alabama/\n",
      "  inflating: data/merged_database/local_govt_in_alabama/local_govt_in_alabama.sqlite  \n",
      "  inflating: data/merged_database/local_govt_in_alabama/schema.sql  \n",
      "   creating: data/merged_database/local_govt_mdm/\n",
      "  inflating: data/merged_database/local_govt_mdm/local_govt_mdm.sqlite  \n",
      "  inflating: data/merged_database/local_govt_mdm/schema.sql  \n",
      "   creating: data/merged_database/machine_repair/\n",
      "  inflating: data/merged_database/machine_repair/machine_repair.sqlite  \n",
      "  inflating: data/merged_database/machine_repair/schema.sql  \n",
      "   creating: data/merged_database/manufactory_1/\n",
      "  inflating: data/merged_database/manufactory_1/manufactory_1.sqlite  \n",
      "  inflating: data/merged_database/manufactory_1/schema.sql  \n",
      "   creating: data/merged_database/manufacturer/\n",
      "  inflating: data/merged_database/manufacturer/manufacturer.sqlite  \n",
      "  inflating: data/merged_database/manufacturer/schema.sql  \n",
      "   creating: data/merged_database/match_season/\n",
      "  inflating: data/merged_database/match_season/match_season.sqlite  \n",
      "  inflating: data/merged_database/match_season/schema.sql  \n",
      "   creating: data/merged_database/medicine_enzyme_interaction/\n",
      "  inflating: data/merged_database/medicine_enzyme_interaction/medicine_enzyme_interaction.sqlite  \n",
      "  inflating: data/merged_database/medicine_enzyme_interaction/schema.sql  \n",
      "   creating: data/merged_database/mountain_photos/\n",
      "  inflating: data/merged_database/mountain_photos/mountain_photos.sqlite  \n",
      "  inflating: data/merged_database/mountain_photos/schema.sql  \n",
      "   creating: data/merged_database/movie_1/\n",
      "  inflating: data/merged_database/movie_1/movie_1.sqlite  \n",
      "  inflating: data/merged_database/movie_1/schema.sql  \n",
      "   creating: data/merged_database/museum_visit/\n",
      "  inflating: data/merged_database/museum_visit/museum_visit.sqlite  \n",
      "  inflating: data/merged_database/museum_visit/schema.sql  \n",
      "   creating: data/merged_database/musical/\n",
      "  inflating: data/merged_database/musical/musical.sqlite  \n",
      "  inflating: data/merged_database/musical/schema.sql  \n",
      "   creating: data/merged_database/music_1/\n",
      "  inflating: data/merged_database/music_1/music_1.sqlite  \n",
      "  inflating: data/merged_database/music_1/schema.sql  \n",
      "   creating: data/merged_database/music_2/\n",
      "  inflating: data/merged_database/music_2/music_2.sqlite  \n",
      "  inflating: data/merged_database/music_2/schema.sql  \n",
      "   creating: data/merged_database/music_4/\n",
      "  inflating: data/merged_database/music_4/music_4.sqlite  \n",
      "  inflating: data/merged_database/music_4/schema.sql  \n",
      "   creating: data/merged_database/network_1/\n",
      "  inflating: data/merged_database/network_1/network_1.sqlite  \n",
      "  inflating: data/merged_database/network_1/schema.sql  \n",
      "   creating: data/merged_database/network_2/\n",
      "  inflating: data/merged_database/network_2/network_2.sqlite  \n",
      "  inflating: data/merged_database/network_2/schema.sql  \n",
      "   creating: data/merged_database/news_report/\n",
      "  inflating: data/merged_database/news_report/news_report.sqlite  \n",
      "  inflating: data/merged_database/news_report/schema.sql  \n",
      "   creating: data/merged_database/orchestra/\n",
      "  inflating: data/merged_database/orchestra/orchestra.sqlite  \n",
      "  inflating: data/merged_database/orchestra/schema.sql  \n",
      "   creating: data/merged_database/party_host/\n",
      "  inflating: data/merged_database/party_host/party_host.sqlite  \n",
      "  inflating: data/merged_database/party_host/schema.sql  \n",
      "   creating: data/merged_database/party_people/\n",
      "  inflating: data/merged_database/party_people/party_people.sqlite  \n",
      "  inflating: data/merged_database/party_people/schema.sql  \n",
      "   creating: data/merged_database/performance_attendance/\n",
      "  inflating: data/merged_database/performance_attendance/performance_attendance.sqlite  \n",
      "  inflating: data/merged_database/performance_attendance/schema.sql  \n",
      "   creating: data/merged_database/perpetrator/\n",
      "  inflating: data/merged_database/perpetrator/perpetrator.sqlite  \n",
      "  inflating: data/merged_database/perpetrator/schema.sql  \n",
      "   creating: data/merged_database/pets_1/\n",
      "  inflating: data/merged_database/pets_1/pets_1.sqlite  \n",
      "  inflating: data/merged_database/pets_1/schema.sql  \n",
      "   creating: data/merged_database/phone_1/\n",
      "  inflating: data/merged_database/phone_1/phone_1.sqlite  \n",
      "  inflating: data/merged_database/phone_1/schema.sql  \n",
      "   creating: data/merged_database/phone_market/\n",
      "  inflating: data/merged_database/phone_market/phone_market.sqlite  \n",
      "  inflating: data/merged_database/phone_market/schema.sql  \n",
      "   creating: data/merged_database/pilot_record/\n",
      "  inflating: data/merged_database/pilot_record/pilot_record.sqlite  \n",
      "  inflating: data/merged_database/pilot_record/schema.sql  \n",
      "   creating: data/merged_database/poker_player/\n",
      "  inflating: data/merged_database/poker_player/poker_player.sqlite  \n",
      "  inflating: data/merged_database/poker_player/schema.sql  \n",
      "   creating: data/merged_database/products_for_hire/\n",
      "  inflating: data/merged_database/products_for_hire/products_for_hire.sqlite  \n",
      "  inflating: data/merged_database/products_for_hire/schema.sql  \n",
      "   creating: data/merged_database/products_gen_characteristics/\n",
      "  inflating: data/merged_database/products_gen_characteristics/products_gen_characteristics.sqlite  \n",
      "  inflating: data/merged_database/products_gen_characteristics/schema.sql  \n",
      "   creating: data/merged_database/product_catalog/\n",
      "  inflating: data/merged_database/product_catalog/product_catalog.sqlite  \n",
      "  inflating: data/merged_database/product_catalog/schema.sql  \n",
      "   creating: data/merged_database/program_share/\n",
      "  inflating: data/merged_database/program_share/program_share.sqlite  \n",
      "  inflating: data/merged_database/program_share/schema.sql  \n",
      "   creating: data/merged_database/protein_institute/\n",
      "  inflating: data/merged_database/protein_institute/protein_institute.sqlite  \n",
      "  inflating: data/merged_database/protein_institute/schema.sql  \n",
      "   creating: data/merged_database/race_track/\n",
      "  inflating: data/merged_database/race_track/race_track.sqlite  \n",
      "  inflating: data/merged_database/race_track/schema.sql  \n",
      "   creating: data/merged_database/railway/\n",
      "  inflating: data/merged_database/railway/railway.sqlite  \n",
      "  inflating: data/merged_database/railway/schema.sql  \n",
      "  inflating: data/merged_database/RatSql.ipynb  \n",
      "   creating: data/merged_database/real_estate_properties/\n",
      "  inflating: data/merged_database/real_estate_properties/real_estate_properties.sqlite  \n",
      "  inflating: data/merged_database/real_estate_properties/schema.sql  \n",
      "   creating: data/merged_database/restaurants/\n",
      "  inflating: data/merged_database/restaurants/restaurants.sqlite  \n",
      "  inflating: data/merged_database/restaurants/schema.sql  \n",
      "   creating: data/merged_database/restaurant_1/\n",
      "  inflating: data/merged_database/restaurant_1/restaurant_1.sqlite  \n",
      "  inflating: data/merged_database/restaurant_1/schema.sql  \n",
      "   creating: data/merged_database/riding_club/\n",
      "  inflating: data/merged_database/riding_club/riding_club.sqlite  \n",
      "  inflating: data/merged_database/riding_club/schema.sql  \n",
      "   creating: data/merged_database/roller_coaster/\n",
      "  inflating: data/merged_database/roller_coaster/roller_coaster.sqlite  \n",
      "  inflating: data/merged_database/roller_coaster/schema.sql  \n",
      "   creating: data/merged_database/sakila_1/\n",
      "  inflating: data/merged_database/sakila_1/sakila_1.sqlite  \n",
      "  inflating: data/merged_database/sakila_1/schema.sql  \n",
      "   creating: data/merged_database/scholar/\n",
      "  inflating: data/merged_database/scholar/schema.sql  \n",
      "  inflating: data/merged_database/scholar/scholar.sqlite  \n",
      "   creating: data/merged_database/school_bus/\n",
      "  inflating: data/merged_database/school_bus/schema.sql  \n",
      "  inflating: data/merged_database/school_bus/school_bus.sqlite  \n",
      "   creating: data/merged_database/school_finance/\n",
      "  inflating: data/merged_database/school_finance/schema.sql  \n",
      "  inflating: data/merged_database/school_finance/school_finance.sqlite  \n",
      "   creating: data/merged_database/school_player/\n",
      "  inflating: data/merged_database/school_player/schema.sql  \n",
      "  inflating: data/merged_database/school_player/school_player.sqlite  \n",
      "   creating: data/merged_database/scientist_1/\n",
      "  inflating: data/merged_database/scientist_1/schema.sql  \n",
      "  inflating: data/merged_database/scientist_1/scientist_1.sqlite  \n",
      "   creating: data/merged_database/ship_1/\n",
      "  inflating: data/merged_database/ship_1/schema.sql  \n",
      "  inflating: data/merged_database/ship_1/ship_1.sqlite  \n",
      "   creating: data/merged_database/ship_mission/\n",
      "  inflating: data/merged_database/ship_mission/schema.sql  \n",
      "  inflating: data/merged_database/ship_mission/ship_mission.sqlite  \n",
      "   creating: data/merged_database/shop_membership/\n",
      "  inflating: data/merged_database/shop_membership/schema.sql  \n",
      "  inflating: data/merged_database/shop_membership/shop_membership.sqlite  \n",
      "   creating: data/merged_database/singer/\n",
      "  inflating: data/merged_database/singer/schema.sql  \n",
      "  inflating: data/merged_database/singer/singer.sqlite  \n",
      "   creating: data/merged_database/small_bank_1/\n",
      "  inflating: data/merged_database/small_bank_1/small_bank_1.sqlite  \n",
      "   creating: data/merged_database/soccer_1/\n",
      "  inflating: data/merged_database/soccer_1/schema.sql  \n",
      "  inflating: data/merged_database/soccer_1/soccer_1.sqlite  \n",
      "   creating: data/merged_database/soccer_2/\n",
      "  inflating: data/merged_database/soccer_2/schema.sql  \n",
      "  inflating: data/merged_database/soccer_2/soccer_2.sqlite  \n",
      "   creating: data/merged_database/solvency_ii/\n",
      "  inflating: data/merged_database/solvency_ii/schema.sql  \n",
      "  inflating: data/merged_database/solvency_ii/solvency_ii.sqlite  \n",
      "   creating: data/merged_database/sports_competition/\n",
      "  inflating: data/merged_database/sports_competition/schema.sql  \n",
      "  inflating: data/merged_database/sports_competition/sports_competition.sqlite  \n",
      "   creating: data/merged_database/station_weather/\n",
      "  inflating: data/merged_database/station_weather/schema.sql  \n",
      "  inflating: data/merged_database/station_weather/station_weather.sqlite  \n",
      "   creating: data/merged_database/store_1/\n",
      "  inflating: data/merged_database/store_1/schema.sql  \n",
      "  inflating: data/merged_database/store_1/store_1.sqlite  \n",
      "   creating: data/merged_database/store_product/\n",
      "  inflating: data/merged_database/store_product/schema.sql  \n",
      "  inflating: data/merged_database/store_product/store_product.sqlite  \n",
      "   creating: data/merged_database/storm_record/\n",
      "  inflating: data/merged_database/storm_record/schema.sql  \n",
      "  inflating: data/merged_database/storm_record/storm_record.sqlite  \n",
      "   creating: data/merged_database/student_1/\n",
      "  inflating: data/merged_database/student_1/annotation.json  \n",
      "   creating: data/merged_database/student_1/data_csv/\n",
      "  inflating: data/merged_database/student_1/data_csv/list.csv  \n",
      "  inflating: data/merged_database/student_1/data_csv/README.STUDENTS.TXT  \n",
      "  inflating: data/merged_database/student_1/data_csv/teachers.csv  \n",
      " extracting: data/merged_database/student_1/link.txt  \n",
      "  inflating: data/merged_database/student_1/q.txt  \n",
      "  inflating: data/merged_database/student_1/student_1.sql  \n",
      "  inflating: data/merged_database/student_1/student_1.sqlite  \n",
      "   creating: data/merged_database/student_assessment/\n",
      "  inflating: data/merged_database/student_assessment/schema.sql  \n",
      "  inflating: data/merged_database/student_assessment/student_assessment.sqlite  \n",
      "   creating: data/merged_database/student_transcripts_tracking/\n",
      "  inflating: data/merged_database/student_transcripts_tracking/schema.sql  \n",
      "  inflating: data/merged_database/student_transcripts_tracking/student_transcripts_tracking.sqlite  \n",
      "   creating: data/merged_database/swimming/\n",
      "  inflating: data/merged_database/swimming/schema.sql  \n",
      "  inflating: data/merged_database/swimming/swimming.sqlite  \n",
      "   creating: data/merged_database/theme_gallery/\n",
      "  inflating: data/merged_database/theme_gallery/schema.sql  \n",
      "  inflating: data/merged_database/theme_gallery/theme_gallery.sqlite  \n",
      "   creating: data/merged_database/tracking_grants_for_research/\n",
      "  inflating: data/merged_database/tracking_grants_for_research/schema.sql  \n",
      "  inflating: data/merged_database/tracking_grants_for_research/tracking_grants_for_research.sqlite  \n",
      "   creating: data/merged_database/tracking_orders/\n",
      "  inflating: data/merged_database/tracking_orders/schema.sql  \n",
      "  inflating: data/merged_database/tracking_orders/tracking_orders.sqlite  \n",
      "   creating: data/merged_database/tracking_share_transactions/\n",
      "  inflating: data/merged_database/tracking_share_transactions/schema.sql  \n",
      "  inflating: data/merged_database/tracking_share_transactions/tracking_share_transactions.sqlite  \n",
      "   creating: data/merged_database/tracking_software_problems/\n",
      "  inflating: data/merged_database/tracking_software_problems/schema.sql  \n",
      "  inflating: data/merged_database/tracking_software_problems/tracking_software_problems.sqlite  \n",
      "   creating: data/merged_database/train_station/\n",
      "  inflating: data/merged_database/train_station/schema.sql  \n",
      "  inflating: data/merged_database/train_station/train_station.sqlite  \n",
      "   creating: data/merged_database/tvshow/\n",
      "  inflating: data/merged_database/tvshow/schema.sql  \n",
      "  inflating: data/merged_database/tvshow/tvshow.sqlite  \n",
      "   creating: data/merged_database/twitter_1/\n",
      "   creating: data/merged_database/twitter_1/queries/\n",
      "  inflating: data/merged_database/twitter_1/queries/oracle-dialects.xml  \n",
      "  inflating: data/merged_database/twitter_1/queries/postgres-dialects.xml  \n",
      "  inflating: data/merged_database/twitter_1/queries/sqlserver-dialects.xml  \n",
      "  inflating: data/merged_database/twitter_1/twitter_1.sqlite  \n",
      "   creating: data/merged_database/university_basketball/\n",
      "  inflating: data/merged_database/university_basketball/schema.sql  \n",
      "  inflating: data/merged_database/university_basketball/university_basketball.sqlite  \n",
      "   creating: data/merged_database/voter_1/\n",
      "  inflating: data/merged_database/voter_1/voter_1.sqlite  \n",
      "   creating: data/merged_database/voter_2/\n",
      "  inflating: data/merged_database/voter_2/schema.sql  \n",
      "  inflating: data/merged_database/voter_2/voter_2.sqlite  \n",
      "   creating: data/merged_database/wedding/\n",
      "  inflating: data/merged_database/wedding/schema.sql  \n",
      "  inflating: data/merged_database/wedding/wedding.sqlite  \n",
      "   creating: data/merged_database/wine_1/\n",
      "   creating: data/merged_database/wine_1/.ipynb_checkpoints/\n",
      "  inflating: data/merged_database/wine_1/annotation.json  \n",
      "   creating: data/merged_database/wine_1/data_csv/\n",
      "  inflating: data/merged_database/wine_1/data_csv/appellations.csv  \n",
      "  inflating: data/merged_database/wine_1/data_csv/grapes.csv  \n",
      "  inflating: data/merged_database/wine_1/data_csv/README.WINE.txt  \n",
      "  inflating: data/merged_database/wine_1/data_csv/wine.csv  \n",
      " extracting: data/merged_database/wine_1/link.txt  \n",
      "  inflating: data/merged_database/wine_1/q.txt  \n",
      "  inflating: data/merged_database/wine_1/wine_1.sql  \n",
      "  inflating: data/merged_database/wine_1/wine_1.sqlite  \n",
      "   creating: data/merged_database/workshop_paper/\n",
      "  inflating: data/merged_database/workshop_paper/schema.sql  \n",
      "  inflating: data/merged_database/workshop_paper/workshop_paper.sqlite  \n",
      "   creating: data/merged_database/world_1/\n",
      "  inflating: data/merged_database/world_1/world_1.json  \n",
      "  inflating: data/merged_database/world_1/world_1.sqlite  \n",
      "   creating: data/merged_database/wrestler/\n",
      "  inflating: data/merged_database/wrestler/schema.sql  \n",
      "  inflating: data/merged_database/wrestler/wrestler.sqlite  \n",
      "   creating: data/merged_database/wta_1/\n",
      "  inflating: data/merged_database/wta_1/wta_1.sql  \n",
      "  inflating: data/merged_database/wta_1/wta_1.sqlite  \n",
      "   creating: data/merged_database/yelp/\n",
      "  inflating: data/merged_database/yelp/schema.sql  \n",
      "  inflating: data/merged_database/yelp/yelp.sqlite  \n",
      "  inflating: data/Список запросов_db_check_v2.xlsx  \n"
     ]
    }
   ],
   "source": [
    "!gdown --id 1Xjbp207zfCaBxhPgt-STB_RxwNo2TIW2\n",
    "#!sudo apt-get install unzip\n",
    "!unzip merged_database_2022-06-10.zip -d data\n",
    "!rm data/merged_database/.DS_Store\n",
    "!rm data/merged_database/RatSql.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4c5d2163-28e6-4b92-9282-3c84050af4d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['id', 'db_id', 'source', 'type', 'question', 'query', 'sql', 'question_toks', 'query_toks', 'query_toks_no_values', 'masked_query', 'tables'],\n",
      "        num_rows: 6558\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['id', 'db_id', 'source', 'type', 'question', 'query', 'sql', 'question_toks', 'query_toks', 'query_toks_no_values', 'masked_query', 'tables'],\n",
      "        num_rows: 1979\n",
      "    })\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "import sqlite3 as sql\n",
    "import os\n",
    "import re\n",
    "\n",
    "new_column = {}\n",
    "\n",
    "for filename in os.listdir('data/merged_database'):\n",
    "  connection = sql.connect(f'data/merged_database/{filename}/{filename}.sqlite')\n",
    "  cur = connection.cursor()\n",
    "\n",
    "  query = \"SELECT sql FROM sqlite_master WHERE type='table';\"\n",
    "\n",
    "  cur.execute(query)\n",
    "  result = cur.fetchall()\n",
    "  fin = ''\n",
    "  for i in result:\n",
    "    res = i[0].replace(\"\\n\", \" \").replace(\"\\\"\", \"\").replace(\"\\t\", \"\").replace(\"`\", \"\")\n",
    "    res = re.sub('\\s+',' ',res)\n",
    "    fin += res + ' '\n",
    "\n",
    "  new_column[filename] = fin[:-2]\n",
    "\n",
    "change_column = []\n",
    "s = 0\n",
    "for i in dataset['train']:\n",
    "    if i['db_id'] in new_column:\n",
    "        used_tables = ''\n",
    "        for k in new_column[i['db_id']].split('CREATE TABLE')[1:]:\n",
    "            table_name = k.split(\" \")[1]\n",
    "            if table_name[-1] == '(':\n",
    "                table_name = table_name[:-1]\n",
    "            if table_name[0] == '\\'':\n",
    "                table_name = table_name[1:-1]\n",
    "            #print(table_name)\n",
    "            if table_name:\n",
    "                #print(table_name)\n",
    "                if table_name.lower() in i['query'].lower():\n",
    "                    if not used_tables:\n",
    "                        ready_string = 'CREATE TABLE' + k\n",
    "                        used_tables += ready_string.strip()\n",
    "                    else:\n",
    "                        ready_string = 'CREATE TABLE' + k\n",
    "                        used_tables += ', ' + ready_string.strip()\n",
    "        if used_tables:\n",
    "            change_column.append(used_tables)\n",
    "        else:\n",
    "            print(s, 'used_tables is EMPTY!')\n",
    "            print('---------------------------------------')\n",
    "            print(new_column[i['db_id']][:-2])\n",
    "            print('---------------------------------------')\n",
    "            print(i['db_id'])\n",
    "            print('---------------------------------------')\n",
    "            print(i['query'])\n",
    "            print('---------------------------------------')\n",
    "            s += 1\n",
    "\n",
    "\n",
    "dataset['train'] = dataset['train'].add_column('tables', change_column)\n",
    "\n",
    "#change_column = []\n",
    "#for i in dataset['test']:\n",
    "#  if i['db_id'] in new_column:\n",
    "#    change_column.append(new_column[i['db_id']])\n",
    "change_column = []\n",
    "s = 0\n",
    "for i in dataset['test']:\n",
    "    if i['db_id'] in new_column:\n",
    "        used_tables = ''\n",
    "        for k in new_column[i['db_id']].split('CREATE TABLE')[1:]:\n",
    "            table_name = k.split(\" \")[1]\n",
    "            if table_name[-1] == '(':\n",
    "                table_name = table_name[:-1]\n",
    "            if table_name[0] == '\\'':\n",
    "                table_name = table_name[1:-1]\n",
    "            #print(table_name)\n",
    "            if table_name:\n",
    "                #print(table_name)\n",
    "                if table_name.lower() in i['query'].lower():\n",
    "                    if not used_tables:\n",
    "                        ready_string = 'CREATE TABLE' + k\n",
    "                        used_tables += ready_string.strip()\n",
    "                    else:\n",
    "                        ready_string = 'CREATE TABLE' + k\n",
    "                        used_tables += ', ' + ready_string.strip()\n",
    "        if used_tables:\n",
    "            change_column.append(used_tables)\n",
    "        else:\n",
    "            print(s, 'used_tables is EMPTY!')\n",
    "            print('---------------------------------------')\n",
    "            print(new_column[i['db_id']][:-2])\n",
    "            print('---------------------------------------')\n",
    "            print(i['db_id'])\n",
    "            print('---------------------------------------')\n",
    "            print(i['query'])\n",
    "            print('---------------------------------------')\n",
    "            s += 1\n",
    "\n",
    "\n",
    "dataset['test'] = dataset['test'].add_column('tables', change_column)\n",
    "#for i in ['question', 'query', 'tables']:\n",
    "#    for k in ['train', 'test']:\n",
    "#        res = []\n",
    "#        for j in dataset[k][i]:\n",
    "#            res.append(j.lower())\n",
    "\n",
    "#        dataset[k] = dataset[k].remove_columns(i).add_column(i, res)\n",
    "        \n",
    "\n",
    "print(dataset)\n",
    "#print(dataset['train']['tables'][:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "edd42976-6311-4966-b3f4-06c6e85f482b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(dataset['train']['tables'][:5])\n",
    "#print(dataset['train']['query'][:5])\n",
    "#print(dataset['train']['question'][:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "407f3f8f-b5f4-4a80-a23c-bb7933b35336",
   "metadata": {},
   "outputs": [],
   "source": [
    "!rm -rf data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "863da1f1-572b-4da9-9a8e-195098b48fe6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['question', 'query', 'tables'],\n",
      "        num_rows: 6558\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['question', 'query', 'tables'],\n",
      "        num_rows: 1979\n",
      "    })\n",
      "})\n",
      "Перечислите имена, место рождения и возраст руководителей отделов в порядке возрастов.\n"
     ]
    }
   ],
   "source": [
    "dataset = dataset.select_columns(['question', 'query', 'tables'])\n",
    "print(dataset)\n",
    "print(dataset['train']['question'][0])\n",
    "q = dataset['train']['question']\n",
    "s = dataset['train']['query']\n",
    "t = dataset['train']['tables']\n",
    "q1 = dataset['test']['question']\n",
    "s1 = dataset['test']['query']\n",
    "t1 = dataset['test']['tables']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7d6d6f80-4d7d-472f-86ef-74475d26f5c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "k = []\n",
    "for i in s:\n",
    "    k.append(i + '</s>')\n",
    "s = k\n",
    "k = []\n",
    "for i in s1:\n",
    "    k.append(i + '</s>')\n",
    "s1 = k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "982deb7c-55a4-4bc2-b832-9581bae8ecdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!gdown --id 1wXLoIKgT1JFHVPMcK83PGZjP8ErIAHLc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "dec4c296-26d1-4ff6-ba68-d104fd98aede",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Какие книги имеют жанр \"фантастика\" и были выпущены после 2000 года?', 'Какие города являются столицами стран?', 'Какие клиенты сделали заказы в период с 1 января 2021 года по 31 декабря 2021 года?'] 2258\n",
      "[\"SELECT * FROM книги WHERE Жанр = 'фантастика' AND Год_выпуска > 2000;\", \"SELECT * FROM города WHERE Столица = 'да';\", \"SELECT DISTINCT клиенты.Имя AS Клиент FROM клиенты INNER JOIN заказы ON клиенты.ID = заказы.Клиент_ID WHERE заказы.Дата_заказа BETWEEN '2021-01-01' AND '2021-12-31';\"] 2258\n",
      "['CREATE TABLE книги (ID INT, Название TEXT, Жанр TEXT, Год_выпуска INT)', 'CREATE TABLE города (ID INT, Название TEXT, Столица TEXT)', 'CREATE TABLE клиенты (ID INT, Имя TEXT), CREATE TABLE заказы (ID INT, Клиент_ID INT, Дата_заказа DATE)'] 2258\n",
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['question', 'query', 'tables'],\n",
      "        num_rows: 8364\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['question', 'query', 'tables'],\n",
      "        num_rows: 2431\n",
      "    })\n",
      "})\n",
      "['Перечислите имена, место рождения и возраст руководителей отделов в порядке возрастов.', 'Каковы годы создания отделов, которыми руководит секретарь, родившийся в штате Айдахо?', 'Как называются штаты, в которых родилось не менее 3 руководителей?']\n",
      "['SELECT name ,  born_state ,  age FROM head ORDER BY age;</s>', \"SELECT DISTINCT T1.creation FROM department AS T1 JOIN management AS T2 ON T1.department_id  =  T2.department_id JOIN head AS T3 ON T2.head_id  =  T3.head_id WHERE T3.born_state  =  'Айдахо'</s>\", 'SELECT born_state FROM head GROUP BY born_state HAVING count(*)  >=  3;</s>']\n",
      "['CREATE TABLE head ( head_ID int, name text, born_state text, age real, PRIMARY KEY (head_ID) )', 'CREATE TABLE department ( Department_ID int, Name text, Creation text, Ranking int, Budget_in_Billions real, Num_Employees real, PRIMARY KEY (Department_ID) ), CREATE TABLE head ( head_ID int, name text, born_state text, age real, PRIMARY KEY (head_ID) ), CREATE TABLE management ( department_ID int, head_ID int, temporary_acting text, PRIMARY KEY (Department_ID,head_ID), FOREIGN KEY (Department_ID) REFERENCES department(Department_ID), FOREIGN KEY (head_ID) REFERENCES head(head_ID)', 'CREATE TABLE head ( head_ID int, name text, born_state text, age real, PRIMARY KEY (head_ID) )']\n"
     ]
    }
   ],
   "source": [
    "with open (\"ru_sql_tables.txt\", \"r\", encoding=\"utf-8\") as f:\n",
    "    doc = f.readlines()\n",
    "question = []\n",
    "query = []\n",
    "tables = []\n",
    "for i in doc:\n",
    "    if 'Задание:' in i:\n",
    "        question.append(i[8:].strip())\n",
    "    if 'Ответ:' in i:\n",
    "        query.append(i[6:].strip())\n",
    "    if 'Таблица:' in i:\n",
    "        tables.append(i[8:].strip())\n",
    "print(question[:3], len(question))\n",
    "print(query[:3], len(query))\n",
    "print(tables[:3], len(tables))\n",
    "\n",
    "k = []\n",
    "for i in query:\n",
    "    k.append(i + '</s>')\n",
    "query = k\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "dict = {'question': question, 'query': query, 'tables': tables}\n",
    "df = pd.DataFrame(dict)\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "train, test = train_test_split(df, test_size = 0.2)\n",
    "d1 = {'question': q, 'query': s, 'tables': t}\n",
    "d2 = {'question': q1, 'query': s1, 'tables': t1}\n",
    "d1 = pd.DataFrame(d1)\n",
    "d2 = pd.DataFrame(d2)\n",
    "train = pd.concat([d1, train], ignore_index=True)\n",
    "test = pd.concat([d2, test], ignore_index=True)\n",
    "\n",
    "from datasets import Dataset, DatasetDict\n",
    "\n",
    "train = Dataset.from_pandas(train)\n",
    "test = Dataset.from_pandas(test)\n",
    "\n",
    "dataset = DatasetDict()\n",
    "\n",
    "dataset['train'] = train\n",
    "dataset['test'] = test\n",
    "\n",
    "print(dataset)\n",
    "print(dataset['train']['question'][:3])\n",
    "print(dataset['train']['query'][:3])\n",
    "print(dataset['train']['tables'][:3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "4245504f",
   "metadata": {
    "id": "4245504f"
   },
   "outputs": [],
   "source": [
    "max_input_length = 938\n",
    "max_target_length = 128\n",
    "\n",
    "def preprocess_function(examples):\n",
    "    #print(examples)\n",
    "    inputs = [\"sql: \" + examples['tables'][i] + ' question: ' + examples['question'][i] for i in range(len(examples['tables']))]\n",
    "    #print(inputs[:5])\n",
    "    \n",
    "    model_inputs = tokenizer(inputs, max_length=max_input_length, truncation=True)\n",
    "\n",
    "    # Setup the tokenizer for targets\n",
    "    labels = tokenizer(text_target=examples[\"query\"], max_length=max_target_length, truncation=True)\n",
    "    \n",
    "    model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
    "    return model_inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "71754d30",
   "metadata": {
    "id": "71754d30"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 8364/8364 [00:01<00:00, 4333.13 examples/s]\n",
      "Map: 100%|██████████| 2431/2431 [00:00<00:00, 8212.92 examples/s]\n"
     ]
    }
   ],
   "source": [
    "tokenized_datasets = dataset.map(preprocess_function, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "dc45ddd1-ebcc-40f4-9b81-02acdf96a4f5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['input_ids', 'attention_mask', 'labels'],\n",
       "        num_rows: 8364\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['input_ids', 'attention_mask', 'labels'],\n",
       "        num_rows: 2431\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_datasets = tokenized_datasets.select_columns(['input_ids', 'attention_mask', 'labels'])\n",
    "tokenized_datasets\n",
    "#for i in tokenized_datasets['train']['input_ids']\n",
    "#print(max(tokenized_datasets['train']['input_ids'], key=len))\n",
    "#print((len(tokenized_datasets['train']['input_ids'])))\n",
    "#print(tokenized_datasets['train']['input_ids'][0])\n",
    "#print(len(tokenizer.decode(max(tokenized_datasets['train']['input_ids'], key=len))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "59e360bb-0577-414f-a608-45f82bd20946",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8364\n",
      "2431\n"
     ]
    }
   ],
   "source": [
    "#tokenized_datasets['train'] = tokenized_datasets['train'].select((row for row in range(len(tokenized_datasets['train'])) if len(tokenized_datasets['train']['input_ids'][row]) != 1024))\n",
    "#tokenized_datasets\n",
    "ids = []\n",
    "#tokenized_datasets['train']['input_ids']\n",
    "for i, k in enumerate(tokenized_datasets['train']['input_ids']):\n",
    "    if len(k) != 1024:\n",
    "        ids.append(i)\n",
    "print(len(ids))\n",
    "\n",
    "ids2 = []\n",
    "#tokenized_datasets['train']['input_ids']\n",
    "for i, k in enumerate(tokenized_datasets['test']['input_ids']):\n",
    "    if len(k) != 1024:\n",
    "        ids2.append(i)\n",
    "print(len(ids2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "5f4fd2ea-ef4b-45c4-849e-438abdd26f57",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['input_ids', 'attention_mask', 'labels'],\n",
       "        num_rows: 8364\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['input_ids', 'attention_mask', 'labels'],\n",
       "        num_rows: 2431\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_datasets['train'] = tokenized_datasets['train'].select(ids)\n",
    "tokenized_datasets['test'] = tokenized_datasets['test'].select(ids2)\n",
    "tokenized_datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "e70b6e43-ffae-4f0a-8a74-919824335410",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "937\n"
     ]
    }
   ],
   "source": [
    "print(len(max(tokenized_datasets['train']['input_ids'], key=len)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "02afa29e-ed30-491b-a4da-91c17c02552d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#tokenized_datasets.save_to_disk(\"tokenized_datasets_2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "43594271-3190-40ce-8661-8ac133d770ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "#tokenized_datasets = load_dataset('tokenized_datasets.hf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "a80152bd",
   "metadata": {
    "id": "a80152bd"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-09-25 14:32:28.712390: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2023-09-25 14:32:28.751724: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2023-09-25 14:32:28.753126: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-09-25 14:32:29.628259: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForSeq2SeqLM, DataCollatorForSeq2Seq, Seq2SeqTrainingArguments, Seq2SeqTrainer\n",
    "\n",
    "#model = AutoModelForSeq2SeqLM.from_pretrained(model_checkpoint,load_in_8bit=True, device_map='auto')\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(model_checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "f7534f40",
   "metadata": {
    "id": "f7534f40"
   },
   "outputs": [],
   "source": [
    "batch_size = 3\n",
    "model_name = model_checkpoint.split(\"/\")[-1]\n",
    "args = Seq2SeqTrainingArguments(\n",
    "    f\"{model_name}-finetuned-sql-ru\",\n",
    "    evaluation_strategy = \"epoch\",\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=batch_size,\n",
    "    per_device_eval_batch_size=batch_size,\n",
    "    weight_decay=0.01,\n",
    "    num_train_epochs=100,\n",
    "    predict_with_generate=True,\n",
    "    save_total_limit = 3,\n",
    "    save_strategy = \"epoch\",\n",
    "    save_steps = 500,\n",
    "    load_best_model_at_end=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "0ccf06f0",
   "metadata": {
    "id": "0ccf06f0"
   },
   "outputs": [],
   "source": [
    "data_collator = DataCollatorForSeq2Seq(tokenizer, model=model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "e50f53b2",
   "metadata": {
    "id": "e50f53b2"
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "import numpy as np\n",
    "import sacrebleu\n",
    "from evaluate import load\n",
    "\n",
    "rouge = load(\"rouge\")\n",
    "bleu = load(\"sacrebleu\")\n",
    "\n",
    "def postprocess_text(predictions, labels):\n",
    "    predictions = [prediction.strip() for prediction in predictions]\n",
    "    labels = [label.strip() for label in labels]\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    predictions, labels = eval_pred\n",
    "\n",
    "    metric = load(\"exact_match\")\n",
    "\n",
    "    #result = {\"exact_match\": 100 * float(np.array_equal(labels, predictions))}\n",
    "    # exect_match_metric\n",
    "\n",
    "    decoded_preds = tokenizer.batch_decode(predictions, skip_special_tokens=True)\n",
    "    # Replace -100 in the labels as we can't decode them.\n",
    "    labels = np.where(labels != -100, labels, tokenizer.pad_token_id)\n",
    "    decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
    "\n",
    "   # result[\"exect_match_v2\"] = metric.compute(predictions=decoded_preds, references=decoded_labels)\n",
    "\n",
    "    res = bleu.compute(predictions=decoded_preds, references=decoded_labels)\n",
    "\n",
    "    acc = sum([int(decoded_preds[i] == decoded_labels[i]) for i in range(len(decoded_preds))]) / len(decoded_preds)\n",
    "\n",
    "   # if isinstance(decoded_labels[0], list):\n",
    "   #     decoded_labels = [[x for x in decoded_label] for decoded_label in decoded_labels]\n",
    "   # else:\n",
    "    # Need to wrap targets in another list for corpus_bleu.\n",
    "   #     decoded_labels = [decoded_labels]\n",
    "\n",
    "    #bleu_score = sacrebleu.corpus_bleu(decoded_preds, decoded_labels,\n",
    "                                     #smooth_method=\"exp\",\n",
    "                                     #smooth_value=0.0,\n",
    "                                     #force=False,\n",
    "                                     #lowercase=False,\n",
    "                                     #tokenize=\"intl\",\n",
    "                                     #use_effective_order=False)\n",
    "    #result['bleu'] = bleu_score.score\n",
    "\n",
    "    # Rouge expects a newline after each sentence\n",
    "    decoded_preds = [\"\\n\".join(nltk.sent_tokenize(pred.strip())) for pred in decoded_preds]\n",
    "    decoded_labels = [\"\\n\".join(nltk.sent_tokenize(label.strip())) for label in decoded_labels]\n",
    "\n",
    "    # Note that other metrics may not have a `use_aggregator` parameter\n",
    "    # and thus will return a list, computing a metric for each sentence.\n",
    "    result = rouge.compute(predictions=decoded_preds, references=decoded_labels, use_stemmer=True, use_aggregator=True)\n",
    "    # Extract a few results\n",
    "    result = {key: value * 100 for key, value in result.items()}\n",
    "\n",
    "    # Add mean generated length\n",
    "    result['bleu'] = res[\"score\"]\n",
    "    result['accuracy'] = acc\n",
    "    \n",
    "    prediction_lens = [np.count_nonzero(pred != tokenizer.pad_token_id) for pred in predictions]\n",
    "    result[\"gen_len\"] = np.mean(prediction_lens)\n",
    "    result = {k: round(v, 4) for k, v in result.items()}\n",
    "\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "e20fac27",
   "metadata": {
    "id": "e20fac27"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "9570c51e",
   "metadata": {
    "id": "9570c51e",
    "outputId": "8e508c52-db94-4df8-b64f-db6afe9e77b9"
   },
   "outputs": [],
   "source": [
    "trainer = Seq2SeqTrainer(\n",
    "    model,\n",
    "    args,\n",
    "    train_dataset=tokenized_datasets[\"train\"],\n",
    "    eval_dataset=tokenized_datasets[\"test\"],\n",
    "    data_collator=data_collator,\n",
    "    tokenizer=tokenizer,\n",
    "    compute_metrics=compute_metrics\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "cca7c3c3-6e8f-4522-afe9-580ae1bec737",
   "metadata": {},
   "outputs": [],
   "source": [
    "#tokenized_datasets['train']['input_ids'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "83d30af1",
   "metadata": {
    "id": "83d30af1"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "      <iframe id=\"tensorboard-frame-1c80317fa3b1799d\" width=\"100%\" height=\"800\" frameborder=\"0\">\n",
       "      </iframe>\n",
       "      <script>\n",
       "        (function() {\n",
       "          const frame = document.getElementById(\"tensorboard-frame-1c80317fa3b1799d\");\n",
       "          const url = new URL(\"/\", window.location);\n",
       "          const port = 6006;\n",
       "          if (port) {\n",
       "            url.port = port;\n",
       "          }\n",
       "          frame.src = url;\n",
       "        })();\n",
       "      </script>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    }
   ],
   "source": [
    "#import tensorflow\n",
    "#!pip install tensorflow\n",
    "%load_ext tensorboard\n",
    "#!kill 197392\n",
    "#%reload_ext tensorboard\n",
    "%tensorboard --logdir '{model_name}-finetuned-sql-ru'/runs\n",
    "#del /q %TMP%\\.tensorboard-info\\*\n",
    "#!pip install -U tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "07a2fa62",
   "metadata": {
    "id": "07a2fa62",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You're using a GPT2TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='95910' max='278800' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 95910/278800 18:47:34 < 35:50:13, 1.42 it/s, Epoch 34.40/100]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Rouge1</th>\n",
       "      <th>Rouge2</th>\n",
       "      <th>Rougel</th>\n",
       "      <th>Rougelsum</th>\n",
       "      <th>Bleu</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Gen Len</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.478800</td>\n",
       "      <td>0.521193</td>\n",
       "      <td>66.298400</td>\n",
       "      <td>48.332400</td>\n",
       "      <td>64.483400</td>\n",
       "      <td>64.507700</td>\n",
       "      <td>26.552100</td>\n",
       "      <td>0.007400</td>\n",
       "      <td>18.990100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.325100</td>\n",
       "      <td>0.409826</td>\n",
       "      <td>70.971300</td>\n",
       "      <td>57.006900</td>\n",
       "      <td>69.922300</td>\n",
       "      <td>69.951700</td>\n",
       "      <td>32.501000</td>\n",
       "      <td>0.011900</td>\n",
       "      <td>18.989300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.262200</td>\n",
       "      <td>0.381063</td>\n",
       "      <td>71.241700</td>\n",
       "      <td>57.891800</td>\n",
       "      <td>70.246900</td>\n",
       "      <td>70.283500</td>\n",
       "      <td>33.518900</td>\n",
       "      <td>0.011900</td>\n",
       "      <td>18.992200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.209400</td>\n",
       "      <td>0.336466</td>\n",
       "      <td>73.030500</td>\n",
       "      <td>60.684400</td>\n",
       "      <td>72.142400</td>\n",
       "      <td>72.187700</td>\n",
       "      <td>35.496000</td>\n",
       "      <td>0.011900</td>\n",
       "      <td>18.991800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.183100</td>\n",
       "      <td>0.310759</td>\n",
       "      <td>73.720600</td>\n",
       "      <td>61.938200</td>\n",
       "      <td>72.762600</td>\n",
       "      <td>72.823800</td>\n",
       "      <td>37.267000</td>\n",
       "      <td>0.013200</td>\n",
       "      <td>18.989700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.162600</td>\n",
       "      <td>0.301720</td>\n",
       "      <td>73.690500</td>\n",
       "      <td>62.355100</td>\n",
       "      <td>72.837600</td>\n",
       "      <td>72.884700</td>\n",
       "      <td>37.657100</td>\n",
       "      <td>0.013200</td>\n",
       "      <td>18.991400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.152300</td>\n",
       "      <td>0.276855</td>\n",
       "      <td>73.949900</td>\n",
       "      <td>62.889500</td>\n",
       "      <td>73.137000</td>\n",
       "      <td>73.191900</td>\n",
       "      <td>38.215100</td>\n",
       "      <td>0.013200</td>\n",
       "      <td>18.992600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.132100</td>\n",
       "      <td>0.285010</td>\n",
       "      <td>74.159900</td>\n",
       "      <td>63.063400</td>\n",
       "      <td>73.265400</td>\n",
       "      <td>73.300100</td>\n",
       "      <td>38.654100</td>\n",
       "      <td>0.013200</td>\n",
       "      <td>18.991800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.118600</td>\n",
       "      <td>0.270462</td>\n",
       "      <td>74.624200</td>\n",
       "      <td>63.831000</td>\n",
       "      <td>73.774500</td>\n",
       "      <td>73.833400</td>\n",
       "      <td>39.139500</td>\n",
       "      <td>0.012800</td>\n",
       "      <td>18.992600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.106400</td>\n",
       "      <td>0.270402</td>\n",
       "      <td>74.437400</td>\n",
       "      <td>63.798400</td>\n",
       "      <td>73.595100</td>\n",
       "      <td>73.643400</td>\n",
       "      <td>39.442500</td>\n",
       "      <td>0.013200</td>\n",
       "      <td>18.992200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>0.104600</td>\n",
       "      <td>0.274952</td>\n",
       "      <td>74.613900</td>\n",
       "      <td>64.144300</td>\n",
       "      <td>73.799400</td>\n",
       "      <td>73.856500</td>\n",
       "      <td>39.490000</td>\n",
       "      <td>0.012800</td>\n",
       "      <td>18.992200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>0.090600</td>\n",
       "      <td>0.261681</td>\n",
       "      <td>74.959600</td>\n",
       "      <td>64.637000</td>\n",
       "      <td>74.194100</td>\n",
       "      <td>74.250400</td>\n",
       "      <td>39.855600</td>\n",
       "      <td>0.012800</td>\n",
       "      <td>18.992200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>0.081100</td>\n",
       "      <td>0.274418</td>\n",
       "      <td>75.097600</td>\n",
       "      <td>65.003500</td>\n",
       "      <td>74.281800</td>\n",
       "      <td>74.337600</td>\n",
       "      <td>40.363500</td>\n",
       "      <td>0.012800</td>\n",
       "      <td>18.992600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>0.077500</td>\n",
       "      <td>0.271044</td>\n",
       "      <td>75.000700</td>\n",
       "      <td>64.652300</td>\n",
       "      <td>74.219200</td>\n",
       "      <td>74.271000</td>\n",
       "      <td>40.140500</td>\n",
       "      <td>0.013600</td>\n",
       "      <td>18.992200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>0.075300</td>\n",
       "      <td>0.273400</td>\n",
       "      <td>74.999400</td>\n",
       "      <td>64.662700</td>\n",
       "      <td>74.214000</td>\n",
       "      <td>74.248500</td>\n",
       "      <td>40.448600</td>\n",
       "      <td>0.013200</td>\n",
       "      <td>18.992600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16</td>\n",
       "      <td>0.070800</td>\n",
       "      <td>0.287251</td>\n",
       "      <td>74.902100</td>\n",
       "      <td>64.615400</td>\n",
       "      <td>74.139100</td>\n",
       "      <td>74.180900</td>\n",
       "      <td>40.249300</td>\n",
       "      <td>0.013200</td>\n",
       "      <td>18.993000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17</td>\n",
       "      <td>0.062900</td>\n",
       "      <td>0.284483</td>\n",
       "      <td>75.295400</td>\n",
       "      <td>65.252800</td>\n",
       "      <td>74.559000</td>\n",
       "      <td>74.583600</td>\n",
       "      <td>41.059100</td>\n",
       "      <td>0.014400</td>\n",
       "      <td>18.992200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18</td>\n",
       "      <td>0.060900</td>\n",
       "      <td>0.275416</td>\n",
       "      <td>74.787900</td>\n",
       "      <td>64.608800</td>\n",
       "      <td>74.093500</td>\n",
       "      <td>74.133300</td>\n",
       "      <td>40.511200</td>\n",
       "      <td>0.013600</td>\n",
       "      <td>18.990100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19</td>\n",
       "      <td>0.058700</td>\n",
       "      <td>0.283135</td>\n",
       "      <td>74.866300</td>\n",
       "      <td>64.694800</td>\n",
       "      <td>74.061700</td>\n",
       "      <td>74.107500</td>\n",
       "      <td>40.702500</td>\n",
       "      <td>0.013600</td>\n",
       "      <td>18.992600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>0.054600</td>\n",
       "      <td>0.301668</td>\n",
       "      <td>74.931000</td>\n",
       "      <td>64.929700</td>\n",
       "      <td>74.206500</td>\n",
       "      <td>74.250200</td>\n",
       "      <td>40.708000</td>\n",
       "      <td>0.014000</td>\n",
       "      <td>18.992200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>21</td>\n",
       "      <td>0.050500</td>\n",
       "      <td>0.316475</td>\n",
       "      <td>75.127300</td>\n",
       "      <td>65.106000</td>\n",
       "      <td>74.432100</td>\n",
       "      <td>74.479800</td>\n",
       "      <td>41.057200</td>\n",
       "      <td>0.014000</td>\n",
       "      <td>18.991800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22</td>\n",
       "      <td>0.049000</td>\n",
       "      <td>0.332129</td>\n",
       "      <td>75.073600</td>\n",
       "      <td>65.115000</td>\n",
       "      <td>74.338200</td>\n",
       "      <td>74.392700</td>\n",
       "      <td>41.230600</td>\n",
       "      <td>0.013600</td>\n",
       "      <td>18.991800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>23</td>\n",
       "      <td>0.048100</td>\n",
       "      <td>0.298008</td>\n",
       "      <td>75.031800</td>\n",
       "      <td>65.059800</td>\n",
       "      <td>74.280600</td>\n",
       "      <td>74.319400</td>\n",
       "      <td>41.112000</td>\n",
       "      <td>0.015600</td>\n",
       "      <td>18.982700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>24</td>\n",
       "      <td>0.043900</td>\n",
       "      <td>0.318859</td>\n",
       "      <td>75.489700</td>\n",
       "      <td>65.724100</td>\n",
       "      <td>74.812200</td>\n",
       "      <td>74.877700</td>\n",
       "      <td>41.224400</td>\n",
       "      <td>0.013600</td>\n",
       "      <td>18.991800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25</td>\n",
       "      <td>0.040200</td>\n",
       "      <td>0.342056</td>\n",
       "      <td>74.900000</td>\n",
       "      <td>64.853300</td>\n",
       "      <td>74.192500</td>\n",
       "      <td>74.249600</td>\n",
       "      <td>41.267400</td>\n",
       "      <td>0.014000</td>\n",
       "      <td>18.990500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>26</td>\n",
       "      <td>0.039100</td>\n",
       "      <td>0.326330</td>\n",
       "      <td>75.326400</td>\n",
       "      <td>65.440900</td>\n",
       "      <td>74.607000</td>\n",
       "      <td>74.654600</td>\n",
       "      <td>41.377900</td>\n",
       "      <td>0.014800</td>\n",
       "      <td>18.988500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>27</td>\n",
       "      <td>0.035800</td>\n",
       "      <td>0.333399</td>\n",
       "      <td>74.823500</td>\n",
       "      <td>64.834200</td>\n",
       "      <td>74.104400</td>\n",
       "      <td>74.164100</td>\n",
       "      <td>40.917600</td>\n",
       "      <td>0.014400</td>\n",
       "      <td>18.991400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>28</td>\n",
       "      <td>0.035600</td>\n",
       "      <td>0.334575</td>\n",
       "      <td>74.821100</td>\n",
       "      <td>64.807400</td>\n",
       "      <td>74.152900</td>\n",
       "      <td>74.196300</td>\n",
       "      <td>40.704600</td>\n",
       "      <td>0.015600</td>\n",
       "      <td>18.991800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>29</td>\n",
       "      <td>0.036900</td>\n",
       "      <td>0.360363</td>\n",
       "      <td>75.506900</td>\n",
       "      <td>65.738400</td>\n",
       "      <td>74.788900</td>\n",
       "      <td>74.843100</td>\n",
       "      <td>41.587000</td>\n",
       "      <td>0.014800</td>\n",
       "      <td>18.989300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>0.033000</td>\n",
       "      <td>0.342903</td>\n",
       "      <td>75.196300</td>\n",
       "      <td>65.230100</td>\n",
       "      <td>74.492600</td>\n",
       "      <td>74.537200</td>\n",
       "      <td>41.103000</td>\n",
       "      <td>0.016900</td>\n",
       "      <td>18.984000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>31</td>\n",
       "      <td>0.032000</td>\n",
       "      <td>0.356167</td>\n",
       "      <td>75.505100</td>\n",
       "      <td>65.805200</td>\n",
       "      <td>74.739900</td>\n",
       "      <td>74.785600</td>\n",
       "      <td>41.428900</td>\n",
       "      <td>0.014800</td>\n",
       "      <td>18.988900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>32</td>\n",
       "      <td>0.029500</td>\n",
       "      <td>0.347030</td>\n",
       "      <td>75.723400</td>\n",
       "      <td>66.071200</td>\n",
       "      <td>74.973700</td>\n",
       "      <td>75.014700</td>\n",
       "      <td>41.885400</td>\n",
       "      <td>0.015600</td>\n",
       "      <td>18.982700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>33</td>\n",
       "      <td>0.028100</td>\n",
       "      <td>0.348749</td>\n",
       "      <td>75.635100</td>\n",
       "      <td>66.101700</td>\n",
       "      <td>74.941800</td>\n",
       "      <td>75.000600</td>\n",
       "      <td>41.702100</td>\n",
       "      <td>0.019300</td>\n",
       "      <td>18.976100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>34</td>\n",
       "      <td>0.027000</td>\n",
       "      <td>0.414275</td>\n",
       "      <td>75.545900</td>\n",
       "      <td>65.969500</td>\n",
       "      <td>74.873200</td>\n",
       "      <td>74.914700</td>\n",
       "      <td>41.395400</td>\n",
       "      <td>0.016500</td>\n",
       "      <td>18.990500</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[31], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/transformers/trainer.py:1553\u001b[0m, in \u001b[0;36mTrainer.train\u001b[0;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m         hf_hub_utils\u001b[38;5;241m.\u001b[39menable_progress_bars()\n\u001b[1;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43minner_training_loop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1554\u001b[0m \u001b[43m        \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1555\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1556\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrial\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1557\u001b[0m \u001b[43m        \u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1558\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/transformers/trainer.py:1835\u001b[0m, in \u001b[0;36mTrainer._inner_training_loop\u001b[0;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[1;32m   1832\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontrol \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcallback_handler\u001b[38;5;241m.\u001b[39mon_step_begin(args, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontrol)\n\u001b[1;32m   1834\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maccelerator\u001b[38;5;241m.\u001b[39maccumulate(model):\n\u001b[0;32m-> 1835\u001b[0m     tr_loss_step \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtraining_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1837\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m   1838\u001b[0m     args\u001b[38;5;241m.\u001b[39mlogging_nan_inf_filter\n\u001b[1;32m   1839\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_torch_tpu_available()\n\u001b[1;32m   1840\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m (torch\u001b[38;5;241m.\u001b[39misnan(tr_loss_step) \u001b[38;5;129;01mor\u001b[39;00m torch\u001b[38;5;241m.\u001b[39misinf(tr_loss_step))\n\u001b[1;32m   1841\u001b[0m ):\n\u001b[1;32m   1842\u001b[0m     \u001b[38;5;66;03m# if loss is nan or inf simply add the average of previous logged losses\u001b[39;00m\n\u001b[1;32m   1843\u001b[0m     tr_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m tr_loss \u001b[38;5;241m/\u001b[39m (\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mglobal_step \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_globalstep_last_logged)\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/transformers/trainer.py:2679\u001b[0m, in \u001b[0;36mTrainer.training_step\u001b[0;34m(self, model, inputs)\u001b[0m\n\u001b[1;32m   2676\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m loss_mb\u001b[38;5;241m.\u001b[39mreduce_mean()\u001b[38;5;241m.\u001b[39mdetach()\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[1;32m   2678\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcompute_loss_context_manager():\n\u001b[0;32m-> 2679\u001b[0m     loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompute_loss\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2681\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mn_gpu \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m   2682\u001b[0m     loss \u001b[38;5;241m=\u001b[39m loss\u001b[38;5;241m.\u001b[39mmean()  \u001b[38;5;66;03m# mean() to average on multi-gpu parallel training\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/transformers/trainer.py:2704\u001b[0m, in \u001b[0;36mTrainer.compute_loss\u001b[0;34m(self, model, inputs, return_outputs)\u001b[0m\n\u001b[1;32m   2702\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   2703\u001b[0m     labels \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m-> 2704\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2705\u001b[0m \u001b[38;5;66;03m# Save past state if it exists\u001b[39;00m\n\u001b[1;32m   2706\u001b[0m \u001b[38;5;66;03m# TODO: this needs to be fixed and made cleaner later.\u001b[39;00m\n\u001b[1;32m   2707\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mpast_index \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/transformers/models/t5/modeling_t5.py:1746\u001b[0m, in \u001b[0;36mT5ForConditionalGeneration.forward\u001b[0;34m(self, input_ids, attention_mask, decoder_input_ids, decoder_attention_mask, head_mask, decoder_head_mask, cross_attn_head_mask, encoder_outputs, past_key_values, inputs_embeds, decoder_inputs_embeds, labels, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m   1743\u001b[0m         decoder_attention_mask \u001b[38;5;241m=\u001b[39m decoder_attention_mask\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdecoder\u001b[38;5;241m.\u001b[39mfirst_device)\n\u001b[1;32m   1745\u001b[0m \u001b[38;5;66;03m# Decode\u001b[39;00m\n\u001b[0;32m-> 1746\u001b[0m decoder_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdecoder\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1747\u001b[0m \u001b[43m    \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdecoder_input_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1748\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdecoder_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1749\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdecoder_inputs_embeds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1750\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1751\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1752\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1753\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdecoder_head_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1754\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcross_attn_head_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcross_attn_head_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1755\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1756\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1757\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1758\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1759\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1761\u001b[0m sequence_output \u001b[38;5;241m=\u001b[39m decoder_outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m   1763\u001b[0m \u001b[38;5;66;03m# Set device for model parallelism\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/transformers/models/t5/modeling_t5.py:1123\u001b[0m, in \u001b[0;36mT5Stack.forward\u001b[0;34m(self, input_ids, attention_mask, encoder_hidden_states, encoder_attention_mask, inputs_embeds, head_mask, cross_attn_head_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m   1110\u001b[0m     layer_outputs \u001b[38;5;241m=\u001b[39m checkpoint(\n\u001b[1;32m   1111\u001b[0m         create_custom_forward(layer_module),\n\u001b[1;32m   1112\u001b[0m         hidden_states,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1120\u001b[0m         \u001b[38;5;28;01mNone\u001b[39;00m,  \u001b[38;5;66;03m# past_key_value is always None with gradient checkpointing\u001b[39;00m\n\u001b[1;32m   1121\u001b[0m     )\n\u001b[1;32m   1122\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1123\u001b[0m     layer_outputs \u001b[38;5;241m=\u001b[39m \u001b[43mlayer_module\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1124\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1125\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mextended_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1126\u001b[0m \u001b[43m        \u001b[49m\u001b[43mposition_bias\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mposition_bias\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1127\u001b[0m \u001b[43m        \u001b[49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1128\u001b[0m \u001b[43m        \u001b[49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoder_extended_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1129\u001b[0m \u001b[43m        \u001b[49m\u001b[43mencoder_decoder_position_bias\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoder_decoder_position_bias\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1130\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlayer_head_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlayer_head_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1131\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcross_attn_layer_head_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcross_attn_layer_head_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1132\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpast_key_value\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_key_value\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1133\u001b[0m \u001b[43m        \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1134\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1135\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1137\u001b[0m \u001b[38;5;66;03m# layer_outputs is a tuple with:\u001b[39;00m\n\u001b[1;32m   1138\u001b[0m \u001b[38;5;66;03m# hidden-states, key-value-states, (self-attention position bias), (self-attention weights), (cross-attention position bias), (cross-attention weights)\u001b[39;00m\n\u001b[1;32m   1139\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m use_cache \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m:\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/transformers/models/t5/modeling_t5.py:755\u001b[0m, in \u001b[0;36mT5Block.forward\u001b[0;34m(self, hidden_states, attention_mask, position_bias, encoder_hidden_states, encoder_attention_mask, encoder_decoder_position_bias, layer_head_mask, cross_attn_layer_head_mask, past_key_value, use_cache, output_attentions, return_dict)\u001b[0m\n\u001b[1;32m    752\u001b[0m     attention_outputs \u001b[38;5;241m=\u001b[39m attention_outputs \u001b[38;5;241m+\u001b[39m cross_attention_outputs[\u001b[38;5;241m2\u001b[39m:]\n\u001b[1;32m    754\u001b[0m \u001b[38;5;66;03m# Apply Feed Forward layer\u001b[39;00m\n\u001b[0;32m--> 755\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlayer\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    757\u001b[0m \u001b[38;5;66;03m# clamp inf values to enable fp16 training\u001b[39;00m\n\u001b[1;32m    758\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m hidden_states\u001b[38;5;241m.\u001b[39mdtype \u001b[38;5;241m==\u001b[39m torch\u001b[38;5;241m.\u001b[39mfloat16:\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/transformers/models/t5/modeling_t5.py:345\u001b[0m, in \u001b[0;36mT5LayerFF.forward\u001b[0;34m(self, hidden_states)\u001b[0m\n\u001b[1;32m    343\u001b[0m forwarded_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayer_norm(hidden_states)\n\u001b[1;32m    344\u001b[0m forwarded_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mDenseReluDense(forwarded_states)\n\u001b[0;32m--> 345\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m hidden_states \u001b[38;5;241m+\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdropout\u001b[49m\u001b[43m(\u001b[49m\u001b[43mforwarded_states\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    346\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m hidden_states\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/torch/nn/modules/dropout.py:59\u001b[0m, in \u001b[0;36mDropout.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m     58\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m---> 59\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdropout\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mp\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtraining\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minplace\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/torch/nn/functional.py:1252\u001b[0m, in \u001b[0;36mdropout\u001b[0;34m(input, p, training, inplace)\u001b[0m\n\u001b[1;32m   1250\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m p \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m0.0\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m p \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1.0\u001b[39m:\n\u001b[1;32m   1251\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdropout probability has to be between 0 and 1, \u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbut got \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(p))\n\u001b[0;32m-> 1252\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m _VF\u001b[38;5;241m.\u001b[39mdropout_(\u001b[38;5;28minput\u001b[39m, p, training) \u001b[38;5;28;01mif\u001b[39;00m inplace \u001b[38;5;28;01melse\u001b[39;00m \u001b[43m_VF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdropout\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mp\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtraining\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "dbcbb866",
   "metadata": {
    "id": "dbcbb866"
   },
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "import torch\n",
    "\n",
    "pipe = pipeline('text2text-generation', model = 'FRED-T5-large-finetuned-sql-ru/checkpoint-33456/', max_length=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "792e729d-cbe1-4a5e-9659-56d844be0efe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Какое изделие имеет наибольшую высоту? Показать название записи в каталоге. \n",
      " SELECT catalog_entry_name FROM catalog_contents ORDER BY height DESC LIMIT 1;</s> \n",
      " CREATE TABLE Catalog_Contents ( catalog_entry_id INTEGER PRIMARY KEY, catalog_level_number INTEGER NOT NULL, parent_entry_id INTEGER, previous_entry_id INTEGER, next_entry_id INTEGER, catalog_entry_name VARCHAR(80), product_stock_number VARCHAR(50), price_in_dollars DOUBLE NULL, price_in_euros DOUBLE NULL, price_in_pounds DOUBLE NULL, capacity VARCHAR(20), length VARCHAR(20), height VARCHAR(20), width VARCHAR(20), FOREIGN KEY (catalog_level_number ) REFERENCES Catalog_Structure(catalog_level_number ) )\n"
     ]
    }
   ],
   "source": [
    "id = 60\n",
    "print(dataset['test']['question'][id], '\\n', dataset['test']['query'][id], '\\n', dataset['test']['tables'][id])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f6d23d9-2df5-458c-99ef-10cd311a1013",
   "metadata": {},
   "outputs": [],
   "source": [
    "#k = 0\n",
    "#for i in dataset['train']['tables']:\n",
    "#    if 'REFERENCES' in i:\n",
    "#        k += 1\n",
    "#k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "54c7c88f",
   "metadata": {
    "id": "54c7c88f",
    "outputId": "b037c034-e2d1-47f6-94d3-ec4b7075f987"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/pechenov.sv@techpark.local/.local/lib/python3.8/site-packages/transformers/generation/configuration_utils.py:399: UserWarning: `num_beams` is set to 1. However, `early_stopping` is set to `True` -- this flag is only used in beam-based generation modes. You should set `num_beams>1` or unset `early_stopping`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[    0,    55, 28289, 23004,    56, 21722,    18,   732,  5059, 27275,\n",
      "         10051,   645,    16, 17165,    18,   630,  2099,   343, 27275, 39817,\n",
      "          1149,    54, 25833, 21722,   907,    50,    50, 11595,  1442,    51,\n",
      "         16961, 17165,  1621,    50, 21722,    18, 23459,  8484, 17165,    18,\n",
      "           684,   326,   645,    67, 23459,  1234,    44, 11595,    41, 17165,\n",
      "            18,   630,  2099,   343,  8484,  6145, 38437,    11,   776,    50,\n",
      "            40, 17165,    18,   684,   326,   645,    67, 23459,  8484, 21722,\n",
      "            18, 23459,    31]])\n",
      "SELECT клиенты.Имя AS Клиент, отзывы.Оценка AS Оценка FROM клиенты INNER JOIN отзывы ON клиенты.ID = отзывы.Клиент_ID WHERE отзывы.Оценка = 'хорошо' AND отзывы.Клиент_ID = клиенты.ID;\n"
     ]
    }
   ],
   "source": [
    "lm_text='sql: CREATE TABLE клиенты (ID INT, Имя TEXT), CREATE TABLE отзывы (ID INT, Клиент_ID INT, Оценка INT) \\\n",
    "question: Какие клиенты оставили отзывы и какие оценки им поставили?'\n",
    "input_ids=torch.tensor([pipe.tokenizer.encode(lm_text)])\n",
    "outputs=pipe.model.generate(input_ids,eos_token_id=pipe.tokenizer.eos_token_id,early_stopping=True, max_length=100)\n",
    "print(outputs)\n",
    "print(pipe.tokenizer.decode(outputs[0][1:]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "96c3c2a0-753c-4530-a447-9fad8f76c538",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CREATE TABLE contacts ( contact_id INTEGER PRIMARY KEY, first_name TEXT NOT NULL, last_name TEXT NOT NULL, email TEXT NOT NULL UNIQUE, phone TEXT NOT NULL UNIQUE )\n",
      "tensor([[    0,    55, 28289, 23004,    56,  8177,    12,    14,    13,  1149,\n",
      "            54, 25833, 38932,    87,  1234,    44, 11595,    41,  3351,    67,\n",
      "            82,  2475,  8484,   458,  1009,  8769,     6,   776,    50,    40,\n",
      "         11541,    67,    82,  2475,  8484,   458, 46928,   299, 13431,     2]])\n",
      "SELECT count(*) FROM contacts WHERE first_name = \"Галина\" AND last_name = \"Какаято\";\n",
      "[(4,)]\n"
     ]
    }
   ],
   "source": [
    "import sqlite3 as sql\n",
    "import re\n",
    "\n",
    "def get_table_data(lm_text, filename):\n",
    "\n",
    "    # Connect to db and take CREATE TABLE structure\n",
    "    connection = sql.connect(f'{filename}.sqlite')\n",
    "    cur = connection.cursor()\n",
    "    \n",
    "    query = \"SELECT sql FROM sqlite_master WHERE type='table';\"\n",
    "    \n",
    "    cur.execute(query)\n",
    "    result = cur.fetchall()\n",
    "\n",
    "    # Get CREATE TABLE structure to needed vue\n",
    "    fin = ''\n",
    "    for i in result:\n",
    "        res = i[0].replace(\"\\n\", \" \").replace(\"\\\"\", \"\").replace(\"\\t\", \"\").replace(\"`\", \"\").replace(\";\", \", \")\n",
    "        res = re.sub('\\s+',' ',res)\n",
    "        fin += res + ', '\n",
    "\n",
    "#    table_data = fin[:-2].lower()\n",
    "    table_data = fin[:-2]\n",
    "\n",
    "    print(table_data)\n",
    "\n",
    "    # Process text-to-sql model\n",
    "    lm_text = 'sql: ' + table_data + ' question: ' + lm_text\n",
    "    input_ids=torch.tensor([pipe.tokenizer.encode(lm_text)])\n",
    "    outputs=pipe.model.generate(input_ids,eos_token_id=pipe.tokenizer.eos_token_id,early_stopping=True, max_length=100)\n",
    "    print(outputs)\n",
    "    sql_query = pipe.tokenizer.decode(outputs[0][1:])\n",
    "\n",
    "#    print(outputs)\n",
    "#    print(sql_query)\n",
    "\n",
    "    #sql_query = sql_query.split(';')[0] + ';'\n",
    "\n",
    "    # Work with capitulation\n",
    "    #match = re.findall(\"[а-яА-Я]*\", sql_query)\n",
    "    #match = [i for i in match if i]\n",
    "    #match_upper = []\n",
    "    #for i in match:\n",
    "    #    i = i.title()\n",
    "    #    match_upper.append(i)\n",
    "    #for i in range(len(match)):\n",
    "    #    sql_query = re.sub(match[i], match_upper[i], sql_query)\n",
    "\n",
    "    #sql_query = \"SELECT * from sqlite_master;\"\n",
    "\n",
    "    print(sql_query[:-4])\n",
    "\n",
    "    # Proceed generated sql query\n",
    "    cur.execute(sql_query[:-4])\n",
    "    result = cur.fetchall()\n",
    "    \n",
    "    return result\n",
    "\n",
    "print(get_table_data('Сколько человек с именем Галина Какаято', 'contacts')) #+3252532\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "600cadd6-61e4-47eb-8595-c842228d10b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(1, 'Дарья', 'Евнухова', '6565@dsfdsf.ru', '+712346346124515'), (2, 'Галина', 'Какаято', '545@dsfdsf.ru', '+732572759239824'), (3, 'Галина', 'Какаято', 'ш7ш7@dsfdsf.ru', '+6547457'), (4, 'Галина', 'Какаято', '2626@dsfdsf.ru', '+6464626'), (5, 'Роман', 'Пухов', '6646@dsfdsf.ru', '+3252532')]\n"
     ]
    }
   ],
   "source": [
    "import sqlite3 as sql\n",
    "\n",
    "connection = sql.connect('contacts1.sqlite')\n",
    "cur = connection.cursor()\n",
    "'''\n",
    "query = \"\"\"CREATE TABLE Contacts (\n",
    "                                  contact_id INTEGER PRIMARY KEY,\n",
    "                                  first_name TEXT NOT NULL,\n",
    "                                  last_name TEXT NOT NULL,\n",
    "                                  email TEXT NOT NULL UNIQUE,\n",
    "                                  phone TEXT NOT NULL UNIQUE\n",
    "                                  );\"\"\"\n",
    "\n",
    "cur.execute(query)\n",
    "result = cur.fetchall()\n",
    "'''\n",
    "'''\n",
    "query = \"\"\"INSERT INTO Contacts (first_name, last_name, email, phone)\n",
    "                        VALUES ('Дарья', 'Евнухова', '6565@dsfdsf.ru', '+712346346124515'),\n",
    "                               ('Галина', 'Какаято', '545@dsfdsf.ru', '+732572759239824'),\n",
    "                               ('Галина', 'Какаято', 'ш7ш7@dsfdsf.ru', '+6547457'),\n",
    "                               ('Галина', 'Какаято', '2626@dsfdsf.ru', '+6464626'),\n",
    "                               ('Роман', 'Пухов', '6646@dsfdsf.ru', '+3252532');\"\"\"\n",
    "\n",
    "\n",
    "cur.execute(query)\n",
    "connection.commit()\n",
    "\n",
    "'''\n",
    "query = 'SELECT * from contacts;'\n",
    "cur.execute(query)\n",
    "result = cur.fetchall()\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "68705671-6aa6-4294-9513-c244284c08fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Дарья', 'Крюкова']\n",
      "select phone from contacts where first_name = \"Дарья\" and last_name  =  \"Крюкова\";\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "str = 'select phone from contacts where first_name = \"дарья\" and last_name  =  \"крюкова\";'\n",
    "\n",
    "match = re.findall(\"[а-я]*\", str)\n",
    "match = [i for i in match if i]\n",
    "match_upper = []\n",
    "for i in match:\n",
    "    i = i.title()\n",
    "    match_upper.append(i)\n",
    "print(match_upper)\n",
    "for i in range(len(match)):\n",
    "    str = re.sub(match[i], match_upper[i], str)\n",
    "print(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "cef2c0b6-0bf5-4a59-9745-fce5831f593e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "/home/pechenov.sv@techpark.local/.local/lib/python3.8/site-packages/gdown/cli.py:126: FutureWarning: Option `--id` was deprecated in version 4.3.1 and will be removed in 5.0. You don't need to pass it anymore to use a file ID.\n",
      "  warnings.warn(\n",
      "Downloading...\n",
      "From: https://drive.google.com/uc?id=1WSvHBZ305E3fqEsUw6Cqo177JLZ5-RfL\n",
      "To: /home/pechenov.sv@techpark.local/report_test.csv\n",
      "100%|██████████████████████████████████████| 26.7k/26.7k [00:00<00:00, 4.86MB/s]\n"
     ]
    }
   ],
   "source": [
    "!gdown --id 1WSvHBZ305E3fqEsUw6Cqo177JLZ5-RfL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "683c284b-456f-41e7-9fb1-0a6fb402919a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "217"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import sqlite3 as sql\n",
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv('report_test.csv')\n",
    "df.to_sql('test', sql.connect('test.sqlite'), if_exists='append', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "6807af66-addb-4b08-8163-8742a406353e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "Token will not been saved to git credential helper. Pass `add_to_git_credential=True` if you want to set the git credential as well.\n",
      "Token is valid (permission: write).\n",
      "Your token has been saved to /home/pechenov.sv@techpark.local/.cache/huggingface/token\n",
      "Login successful\n"
     ]
    }
   ],
   "source": [
    "!huggingface-cli login --token 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"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "50472746-248f-4c93-8645-4d4c9e1fb407",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CommitInfo(commit_url='https://huggingface.co/MadShift/test-model/commit/fdde9e9896372c3e555d0b32742acee1744f144e', commit_message='Upload T5ForConditionalGeneration', commit_description='', oid='fdde9e9896372c3e555d0b32742acee1744f144e', pr_url=None, pr_revision=None, pr_num=None)"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipe.model.push_to_hub(repo_id='MadShift/test-model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "d4c52fa4-799d-411b-8b9c-69a775612459",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CommitInfo(commit_url='https://huggingface.co/MadShift/test-model/commit/d648a1f63b1b041a82953a4be3e999857390fde1', commit_message='Upload tokenizer', commit_description='', oid='d648a1f63b1b041a82953a4be3e999857390fde1', pr_url=None, pr_revision=None, pr_num=None)"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipe.tokenizer.push_to_hub(repo_id='MadShift/test-model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c6a7371",
   "metadata": {
    "id": "7c6a7371"
   },
   "outputs": [],
   "source": [
    "#pipe.model.parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a06819e8",
   "metadata": {
    "id": "a06819e8"
   },
   "outputs": [],
   "source": [
    "#import getpass\n",
    "#import os\n",
    "\n",
    "#password = getpass.getpass()\n",
    "#command = \"sudo -S apt install git-lfs\" #can be any command but don't forget -S as it enables input from stdin\n",
    "#os.system('echo %s | %s' % (password, command))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97aa01bf",
   "metadata": {
    "id": "97aa01bf"
   },
   "outputs": [],
   "source": [
    "#!sudo apt install git-lfs"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": [
    {
     "file_id": "1J1NELU3Wcl1Tnvw0yCXgU5As2C-cvfhZ",
     "timestamp": 1694608434407
    }
   ]
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
